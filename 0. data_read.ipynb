{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import pickle\n",
    "\n",
    "#declare some variables\n",
    "today = pd.Timestamp.today()\n",
    "path_in = 'data/data_lokad'\n",
    "path_out ='data/temp01'\n",
    "\n",
    "## TODO \n",
    "# import reworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purchasing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load current file + format + add data source column\n",
    "PURCHASING = pd.read_csv(path_in + '/LOKAD_PURCHASING.txt',delimiter='|',\n",
    "                           dtype={\n",
    "                               'PN':'str',\n",
    "                               'PNM':'str',\n",
    "                           })\n",
    "PURCHASING['PURDATE'] = pd.to_datetime(PURCHASING['PURDATE'],format='%Y-%m-%d',errors='coerce')\n",
    "PURCHASING['RECDATE'] = pd.to_datetime(PURCHASING['RECDATE'],format='%Y-%m-%d',errors='coerce')\n",
    "PURCHASING['DATA_SOURCE'] = 'SQL'\n",
    "#load Histo PO and format\n",
    "HISTO_PO_ARO = pd.read_csv(path_in + '/HISTORICAL_PURCHASES_ARO.txt',delimiter='|',\n",
    "                            dtype={'PNM':'str','item':'str'},\n",
    "                            usecols=[1,2,3,4,5,6,7,8,9,10,11])\n",
    "HISTO_PO_ARO = HISTO_PO_ARO.loc[HISTO_PO_ARO['QTYPENDING'] == 0] #delete suspicious open POs\n",
    "HISTO_PO_ARO['DEPT_NAME'] = 'PURCHASING ARO'\n",
    "HISTO_PO_ARO['DATA_SOURCE'] = 'HISTO ARO'\n",
    "HISTO_PO_ARO['PURDATE'] = pd.to_datetime(HISTO_PO_ARO['PURDATE'], format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_PO_ARO['RECDATE'] = pd.to_datetime(HISTO_PO_ARO['RECDATE'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "HISTO_PO_BIC = pd.read_csv(path_in + '/HISTORICAL_PURCHASES_BIC.txt',delimiter='|',\n",
    "                          dtype={'PNM':'str'},\n",
    "                          usecols=[1,2,3,4,5,6,7,8,9,10,11])\n",
    "HISTO_PO_BIC = HISTO_PO_BIC.loc[HISTO_PO_BIC['QTYPENDING'] == 0] #delete suspicious open POs\n",
    "HISTO_PO_BIC['DEPT_NAME'] = 'PURCHASING BIC'\n",
    "HISTO_PO_BIC['DATA_SOURCE'] = 'HISTO BIC'\n",
    "HISTO_PO_BIC['PURDATE'] = pd.to_datetime(HISTO_PO_BIC['PURDATE'], format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_PO_BIC['RECDATE'] = pd.to_datetime(HISTO_PO_BIC['RECDATE'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "#append histo data\n",
    "PURCHASING = PURCHASING.append(HISTO_PO_ARO, ignore_index = True, sort = False)\n",
    "PURCHASING = PURCHASING.append(HISTO_PO_BIC, ignore_index = True, sort = False)\n",
    "\n",
    "#cleanining\n",
    "PURCHASING['QTYRECEIVED'] = pd.to_numeric(PURCHASING['QTYRECEIVED'], downcast='integer')\n",
    "PURCHASING['QTYPENDING'] = pd.to_numeric(PURCHASING['QTYPENDING'], downcast='integer')\n",
    "PURCHASING['EXT_COST'] = PURCHASING['COST'] * PURCHASING['QTYRECEIVED']\n",
    "PURCHASING = PURCHASING[~PURCHASING['GEO_CODE'].isnull()] \n",
    "PURCHASING = PURCHASING[(PURCHASING['RECDATE'] <= today)|(PURCHASING['RECDATE']).isnull()] #remove POs received in the futur\n",
    "\n",
    "PURCHASING.to_pickle(path_out+'/PURCHASING.pkl')\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Detail data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK = pd.read_csv(path_in + '/LOKAD_STOCK_DETAIL.txt',delimiter='|',\n",
    "                           dtype={\n",
    "                               'PN':'str',\n",
    "                               'PNM':'str'\n",
    "                           })\n",
    "STOCK['RECDATE'] = pd.to_datetime(STOCK['RECDATE'], format='%Y-%m-%d') #what to do the null ones??\n",
    "#STOCK['UNIT_COST'] = STOCK['UNIT_COST'].str.replace('$','') # change the negative costs ((WTF))\n",
    "#STOCK['UNIT_COST'] = STOCK['UNIT_COST'].str.replace('(','-')\n",
    "#STOCK['UNIT_COST'] = STOCK['UNIT_COST'].str.replace(')','')\n",
    "STOCK['UNIT_COST'] = pd.to_numeric(STOCK['UNIT_COST'], downcast='float')\n",
    "\n",
    "\n",
    "STOCK.to_pickle(path_out + '/STOCK.pkl')\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts Master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTS_MASTER = pd.read_csv(path_in + '/LOKAD_PARTS_MASTER.txt',delimiter='|',\n",
    "                            dtype={\n",
    "                                'PN':'str',\n",
    "                                'PNM':'str',\n",
    "                            })\n",
    "\n",
    "PARTS_MASTER['LIST_PRICE'] = PARTS_MASTER['LIST_PRICE'].str.replace('$','') # change the negative costs ((WTF))\n",
    "PARTS_MASTER['LIST_PRICE'] = PARTS_MASTER['LIST_PRICE'].str.replace('(','-')\n",
    "PARTS_MASTER['LIST_PRICE'] = PARTS_MASTER['LIST_PRICE'].str.replace(')','')\n",
    "PARTS_MASTER['PRICEDATE'] = pd.to_datetime(PARTS_MASTER['PRICEDATE'], format='%Y-%m-%d', errors='coerce')\n",
    "PARTS_MASTER['LIST_PRICE'] = pd.to_numeric(PARTS_MASTER['LIST_PRICE'], downcast='float')\n",
    "\n",
    "PARTS_MASTER.to_pickle(path_out + '/PARTS_MASTER.pkl')\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALTERNATES = pd.read_csv(path_in + '/LOKAD_ALTERNATES.txt',delimiter='|',\n",
    "                            dtype={\n",
    "                                'PNM':'str',\n",
    "                                'ALT_PNM':'str',\n",
    "                                'PN':'str',\n",
    "                                'ALT_PN':'str'\n",
    "                            })\n",
    "\n",
    "ALTERNATES.to_pickle(path_out+'\\ALTERNATES.pkl')\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Orders data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load current file\n",
    "SALES_ORDERS = pd.read_csv(path_in + '/LOKAD_SALES_ORDERS.txt',delimiter='|',\n",
    "                            dtype={\n",
    "                                'PNM':'str',\n",
    "                                'PN':'str'\n",
    "                            })\n",
    "\n",
    "SALES_ORDERS['DATA_SOURCE'] = 'SQL'\n",
    "SALES_ORDERS['ENTRY_DATE'] = pd.to_datetime(SALES_ORDERS['ENTRY_DATE'], format='%Y-%m-%d', errors='coerce')\n",
    "SALES_ORDERS['SHIP_DATE'] = pd.to_datetime(SALES_ORDERS['SHIP_DATE'], format='%Y-%m-%d', errors='coerce')\n",
    "SALES_ORDERS = SALES_ORDERS.drop(columns='ZNEG')\n",
    "\n",
    "#load histo SO and format\n",
    "HISTO_SO_BIC = pd.read_csv(path_in + '/HISTORICAL_SALES_BIC.txt',delimiter='|',\n",
    "                          dtype={'PNM':'str'})\n",
    "HISTO_SO_BIC = HISTO_SO_BIC.drop(columns='ID')\n",
    "HISTO_SO_BIC['ENTRY_DATE'] = pd.to_datetime(HISTO_SO_BIC['ENTRY_DATE'], format = '%m/%d/%Y %H:%M:%S' )\n",
    "HISTO_SO_BIC['SHIP_DATE'] = pd.to_datetime(HISTO_SO_BIC['ENTRY_DATE'], format = '%m/%d/%Y %H:%M:%S' )\n",
    "HISTO_SO_BIC['DATA_SOURCE'] = 'HISTO BIC'\n",
    "\n",
    "#parse histo data\n",
    "SALES_ORDERS = SALES_ORDERS.append(HISTO_SO_BIC, ignore_index = True, sort = False)\n",
    "\n",
    "# save as pickle\n",
    "SALES_ORDERS.to_pickle(path_out + '/SALES_ORDERS.pkl')\n",
    "\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WO Bom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Current file\n",
    "WO_BOM = pd.read_csv(path_in + '/LOKAD_WO_BOM.txt',delimiter='|',\n",
    "                            dtype={\n",
    "                                'PNM':'str',\n",
    "                                'PN':'str'\n",
    "                            })\n",
    "WO_BOM['ADD_DATE'] = pd.to_datetime(WO_BOM['ADD_DATE'],format='%Y-%m-%d',errors='coerce')\n",
    "WO_BOM['ISSUE_DATE'] = pd.to_datetime(WO_BOM['ISSUE_DATE'],format='%Y-%m-%d',errors='coerce')\n",
    "WO_BOM['DATA_SOURCE'] = 'SQL'\n",
    "WO_BOM['EXT_COST'] = WO_BOM['QTY_ISSUED'] * WO_BOM['COST']\n",
    "\n",
    "#load histo WOB and format\n",
    "HISTO_WOB_ARO = pd.read_csv(path_in + '/HISTORICAL_WO_BOM_ARO.txt',delimiter='|',\n",
    "                           dtype={'PNM':'str',\n",
    "                                  'WONO':'str'\n",
    "                                  })\n",
    "HISTO_WOB_ARO = HISTO_WOB_ARO.drop(columns=['ID'])\n",
    "\n",
    "HISTO_WOB_ARO['ADD_DATE'] = pd.to_datetime(HISTO_WOB_ARO['ADD_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOB_ARO['ISSUE_DATE'] = pd.to_datetime(HISTO_WOB_ARO['ISSUE_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOB_ARO['DATA_SOURCE'] = 'HISTO ARO'\n",
    "HISTO_WOB_ARO['GEO_CODE'] = 'DOR' #shouldn't been needed later\n",
    "\n",
    "HISTO_WOB_BIC = pd.read_csv(path_in + '/HISTORICAL_WO_BOM_BIC.txt',delimiter='|',\n",
    "                           dtype={'PNM':'str','WONO':'str'})\n",
    "HISTO_WOB_BIC['ADD_DATE'] = pd.to_datetime(HISTO_WOB_BIC['ADD_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOB_BIC['ISSUE_DATE'] = pd.to_datetime(HISTO_WOB_BIC['ISSUE_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOB_BIC = HISTO_WOB_BIC.drop(columns=['ID'])\n",
    "HISTO_WOB_BIC['DATA_SOURCE'] = 'HISTO BIC'\n",
    "HISTO_WOB_BIC['GEO_CODE'] = 'MIA MRO' #shouldn't been needed later\n",
    "\n",
    "#Append histo to current\n",
    "WO_BOM= WO_BOM.append(HISTO_WOB_ARO, ignore_index = True, sort = False)\n",
    "WO_BOM= WO_BOM.append(HISTO_WOB_BIC, ignore_index = True, sort = False)\n",
    "\n",
    "#Cleaning\n",
    "WO_BOM = WO_BOM[~WO_BOM['ACTIVITY'].str.contains('2014')] #bad conversions from histo data\n",
    "WO_BOM = WO_BOM[~WO_BOM['ACTIVITY'].str.contains('2015')]\n",
    "WO_BOM = WO_BOM[~WO_BOM['ACTIVITY'].str.contains('2016')]\n",
    "WO_BOM = WO_BOM.loc[WO_BOM['ACTIVITY'] != 'CLOSED']  #bad conversions from histo data\n",
    "\n",
    "#save as pickly\n",
    "WO_BOM.to_pickle(path_out + '/WO_BPM.pkl')\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WO Header data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load current file\n",
    "WO_HEADER = pd.read_csv(path_in + '/LOKAD_WO_HEADER.txt',delimiter='|',\n",
    "                            dtype={\n",
    "                                'PNM':'str',\n",
    "                                'PN':'str'\n",
    "                            })\n",
    "WO_HEADER['DATA_SOURCE'] = 'SQL'\n",
    "WO_HEADER['ENTRY_DATE'] = pd.to_datetime(WO_HEADER['ENTRY_DATE'],format='%Y-%m-%d',errors='coerce')\n",
    "WO_HEADER['CLOSE_DATE'] = pd.to_datetime(WO_HEADER['CLOSE_DATE'],format='%Y-%m-%d',errors='coerce')\n",
    "\n",
    "#load histo WOH and format\n",
    "HISTO_WOH_ARO = pd.read_csv(path_in + '/HISTORICAL_WO_HEADER_ARO.txt',delimiter='|',\n",
    "                           dtype={'PNM':'str','WONO':'str'},\n",
    "                           usecols=[1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "HISTO_WOH_ARO['ENTRY_DATE'] = pd.to_datetime(HISTO_WOH_ARO['ENTRY_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOH_ARO['CLOSE_DATE'] = pd.to_datetime(HISTO_WOH_ARO['CLOSE_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOH_ARO['DATA_SOURCE'] = 'HISTO ARO'\n",
    "HISTO_WOH_ARO['GEO_CODE'] = np.where(HISTO_WOH_ARO['GEO_CODE'] == 'MED','MED MRO',HISTO_WOH_ARO['GEO_CODE'])\n",
    "\n",
    "HISTO_WOH_BIC = pd.read_csv(path_in + '/HISTORICAL_WO_HEADER_BIC.txt',delimiter='|',\n",
    "                           dtype={'PNM':'str','WONO':'str'})\n",
    "HISTO_WOH_BIC = HISTO_WOH_BIC.drop(columns=['ID'])\n",
    "HISTO_WOH_BIC['ENTRY_DATE'] = pd.to_datetime(HISTO_WOH_BIC['ENTRY_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOH_BIC['CLOSE_DATE'] = pd.to_datetime(HISTO_WOH_BIC['CLOSE_DATE'],  format='%m/%d/%Y %H:%M:%S')\n",
    "HISTO_WOH_BIC['DATA_SOURCE'] = 'HISTO BIC'\n",
    "\n",
    "\n",
    "#append the 2 histo files together\n",
    "HISTO_WOH = HISTO_WOH_BIC.append(HISTO_WOH_ARO, ignore_index = True, sort = False)\n",
    "\n",
    "#drop all records from histo if WO also exists in current WO_HEADER\n",
    "HISTO_WOH = HISTO_WOH[~HISTO_WOH['WONO'].isin(WO_HEADER['WONO'].unique())]\n",
    "\n",
    "#append histoical data to current data\n",
    "WO_HEADER= WO_HEADER.append(HISTO_WOH, ignore_index = True, sort = False)\n",
    "\n",
    "#clean data\n",
    "WO_HEADER = WO_HEADER.loc[WO_HEADER['GEO_CODE'] != 'TEST']\n",
    "WO_HEADER = WO_HEADER.loc[~WO_HEADER['GEO_CODE'].isnull()]\n",
    "WO_HEADER.drop_duplicates(subset=['WONO','SERIAL_NUMBER'],keep='last',inplace = True)\n",
    "\n",
    "#save as pickle\n",
    "WO_HEADER.to_pickle(path_out + '/WO_HEADER.pkl')\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DOR', nan, 'TEST', 'MED MRO'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HISTO_WOH_ARO['GEO_CODE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPABILITIES = pd.read_csv(path_in + '/ba_view_internal_capabilities.txt',delimiter='|')\n",
    "CAPABILITIES['IS_ACTIVE'] = np.where(CAPABILITIES['CAPABILITY_CODE'].str.contains('INACTIVE'),\n",
    "                                     '0', '1') #adds colunm to show active caps\n",
    "\n",
    "#CAPABILITIES\n",
    "CAPABILITIES = CAPABILITIES[CAPABILITIES['CAPABILITY_CODE'] != 'Barfield, Inc.']\n",
    "CAPABILITIES = CAPABILITIES[~CAPABILITIES['CAPABILITY_CODE'].isnull()]\n",
    "CAPABILITIES = CAPABILITIES.drop(columns = ['APPLICATION_CODE','DESCRIPTION',\n",
    "                                            'MFG_CODE','MFG_NAME','STOCK_CATEGORY_CODE',\n",
    "                                            'CAPABILITY_CODE','EST_COST','ACTIVE_MANUALS',\n",
    "                                            'SYSCM_AUTO_KEY','SYCPC_AUTO_KEY','INACTIVE_MANUALS'\n",
    "                                           ])\n",
    "CAPABILITIES.rename(columns = {'PNM_AUTO_KEY':'PNM'}, inplace = True)\n",
    "CAPABILITIES['IS_ACTIVE'] = pd.to_numeric(CAPABILITIES['IS_ACTIVE'], downcast='integer')\n",
    "\n",
    "\n",
    "#sum \"active\" field in case they exist in both condition - by default show them active\n",
    "CAPABILITIES = pd.pivot_table(CAPABILITIES, values ='IS_ACTIVE', aggfunc=np.sum,index=['PN','GEO_CODE']).reset_index()\n",
    "CAPABILITIES['IS_ACTIVE'] = np.where(CAPABILITIES['IS_ACTIVE']>=1, 1, 0)\n",
    "\n",
    "#save as pickle\n",
    "CAPABILITIES.to_pickle(path_out + '/CAPABILITIES.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: verif charts / numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All other Static files (not sure that this is useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIB_PARTS = pd.read_csv(path_in + '/BICdistributedparts.csv',delimiter=';',\n",
    "                                dtype={\n",
    "                                    'PNM':'str',\n",
    "                                    'PN':'str'\n",
    "                                })\n",
    "EXCLUD_PARTS_SO = pd.read_csv(path_in + '/ExcludedPartsForSalesOrders.csv',\n",
    "                              delimiter=',',\n",
    "                              dtype={\n",
    "                                     'PN':'str' \n",
    "                              })\n",
    "EXCLUD_CLIENT_SO = pd.read_csv(path_in + '/ExcludedClientsForSalesOrders.csv',\n",
    "                               delimiter=',')\n",
    "EXCLUD_SO = pd.read_csv(path_in + '/ExcludedSalesOrders.csv',delimiter=',')\n",
    "EXCLUD_PARTS = pd.read_csv(path_in + '/PartsToBeExcluded.csv',delimiter=',')\n",
    "MANUAL_MIN_MAX = pd.read_csv(path_in + '/min_max_static.csv',delimiter=',',\n",
    "                            dtype={\n",
    "                                'PN':'str'\n",
    "                            })\n",
    "PMA_ACCEPTANCE = pd.read_csv(path_in + '/pma_acceptance.csv',delimiter=',',\n",
    "                            dtype={'PN':'str'})\n",
    "VIP_LRU = pd.read_csv(path_in + '/VIP_LRUs.csv',delimiter=',')\n",
    "WHS = pd.read_csv(path_in + '/WHS.csv',delimiter=',') # this one shouldn't be static -\n",
    "                                                      #need to SQL it as well as the DEPT one\n",
    "WON_BIZ = pd.read_csv(path_in + '/WON_BUSINESS.csv',delimiter=',')\n",
    "LOST_BIZ = pd.read_csv(path_in + '/LOST_BUSINESS.csv',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
